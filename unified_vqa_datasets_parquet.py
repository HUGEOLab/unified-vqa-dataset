#!/usr/bin/env python3
"""
Parquet + Streaming ç‰ˆæœ¬ä¸Šä¼ è„šæœ¬ï¼š
- è‡ªåŠ¨å°†å›¾ç‰‡ç›®å½•è½¬æ¢ä¸º Parquet æ ¼å¼
- æ”¯æŒå¢é‡æ›´æ–°ï¼ˆæ™ºèƒ½æ£€æµ‹å˜åŒ–ï¼‰
- ç”Ÿæˆ Streaming å‹å¥½çš„æ•°æ®é›†
"""

import os
import json
import subprocess
from pathlib import Path
from typing import List, Tuple, Dict, Optional
from datasets import Dataset, Features, Image, Value, load_dataset
from huggingface_hub import HfApi, login
from PIL import Image as PILImage


# ============ é…ç½®ä¿¡æ¯ ============
CURRENT_DIR = Path("/mnt/mydev2/M256374/unified-vqa-dataset")
HF_REPO = "Geojx/unified-vqa-images"
HF_BRANCH = "main"
GITHUB_REPO = "HUGEOLab/unified-vqa-dataset"
GITHUB_BRANCH = "main"

# å›¾ç‰‡æ‰©å±•å
IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp'}

# â­ æ–°å¢ï¼šæ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
ANNOTATION_FILE = CURRENT_DIR / "annotations.json"  # æˆ– .csv


def run_cmd(cmd, cwd=None, check=True):
    """è¿è¡Œç³»ç»Ÿå‘½ä»¤"""
    try:
        result = subprocess.run(
            cmd, cwd=cwd, check=check, 
            stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True
        )
        return True, result.stdout
    except subprocess.CalledProcessError as e:
        return False, e.stderr


def load_annotations() -> Optional[Dict]:
    """
    åŠ è½½æ ‡æ³¨æ•°æ®ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    æ”¯æŒæ ¼å¼ï¼š
    1. JSON: {"image_id": {"question": "...", "answer": "..."}, ...}
    2. CSV: image_id,question,answer
    """
    if not ANNOTATION_FILE.exists():
        print("âš ï¸ æœªæ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶ï¼Œå°†ä»…æ‰“åŒ…å›¾ç‰‡")
        return None
    
    print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ ‡æ³¨: {ANNOTATION_FILE}")
    
    if ANNOTATION_FILE.suffix == ".json":
        with open(ANNOTATION_FILE) as f:
            return json.load(f)
    
    elif ANNOTATION_FILE.suffix == ".csv":
        import pandas as pd
        df = pd.read_csv(ANNOTATION_FILE)
        # è½¬æ¢æˆå­—å…¸ {image_id: {å…¶ä»–åˆ—}}
        return df.set_index('image_id').to_dict('index')
    
    return None


def scan_images() -> List[Dict]:
    """
    æ‰«æå›¾ç‰‡ç›®å½•ï¼Œæ„å»ºæ•°æ®é›†åˆ—è¡¨
    è¿”å›æ ¼å¼: [{"image": PIL.Image, "image_id": "...", ...}, ...]
    """
    images_dir = CURRENT_DIR / "unified-vqa-images"
    
    if not images_dir.exists():
        raise FileNotFoundError(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
    
    print(f"ğŸ” æ‰«æå›¾ç‰‡ç›®å½•: {images_dir}")
    
    # åŠ è½½æ ‡æ³¨
    annotations = load_annotations()
    
    data_list = []
    
    for root, _, files in os.walk(images_dir):
        for file in sorted(files):  # æ’åºä¿è¯å¯é‡å¤æ€§
            if Path(file).suffix.lower() not in IMAGE_EXTENSIONS:
                continue
            
            image_path = Path(root) / file
            image_id = image_path.stem  # å»æ‰æ‰©å±•åä½œä¸º ID
            
            # æ„å»ºæ•°æ®æ¡ç›®
            item = {
                "image": str(image_path),  # datasets åº“ä¼šè‡ªåŠ¨å¤„ç†è·¯å¾„
                "image_id": image_id,
            }
            
            # å¦‚æœæœ‰æ ‡æ³¨ï¼Œæ·»åŠ å¯¹åº”å­—æ®µ
            if annotations and image_id in annotations:
                item.update(annotations[image_id])
            
            data_list.append(item)
    
    print(f"âœ… æ‰¾åˆ° {len(data_list)} å¼ å›¾ç‰‡")
    return data_list


def create_parquet_dataset():
    """
    æ ¸å¿ƒå‡½æ•°ï¼šå°†å›¾ç‰‡ + æ ‡æ³¨è½¬æ¢ä¸º Parquet æ ¼å¼æ•°æ®é›†
    """
    print("\n" + "="*60)
    print("ğŸ”„ å¼€å§‹è½¬æ¢ä¸º Parquet æ ¼å¼...")
    print("="*60)
    
    # 1. æ‰«æå›¾ç‰‡
    data_list = scan_images()
    
    if not data_list:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å›¾ç‰‡ï¼")
    
    # 2. å®šä¹‰ Schemaï¼ˆæ•°æ®ç»“æ„ï¼‰
    # âš ï¸ é‡è¦ï¼šImage() ç±»å‹ä¼šè‡ªåŠ¨å¤„ç†äºŒè¿›åˆ¶å­˜å‚¨
    features = Features({
        "image": Image(),  # è¿™æ˜¯å…³é”®ï¼è‡ªåŠ¨æ— æŸå­˜å‚¨
        "image_id": Value("string"),
        # å¦‚æœæœ‰æ ‡æ³¨ï¼Œæ·»åŠ å¯¹åº”å­—æ®µ
        # "question": Value("string"),
        # "answer": Value("string"),
    })
    
    # åŠ¨æ€æ£€æµ‹æ ‡æ³¨å­—æ®µ
    if data_list[0].keys() - {"image", "image_id"}:
        extra_fields = data_list[0].keys() - {"image", "image_id"}
        for field in extra_fields:
            features[field] = Value("string")  # æ ¹æ®å®é™…ç±»å‹è°ƒæ•´
    
    # 3. åˆ›å»º Dataset å¯¹è±¡
    print("   ğŸ“¦ æ­£åœ¨æ‰“åŒ…æ•°æ®...")
    dataset = Dataset.from_list(data_list, features=features)
    
    # 4. ï¼ˆå¯é€‰ï¼‰éªŒè¯ä¸€å¼ å›¾ç‰‡çš„å®Œæ•´æ€§
    print("\nğŸ”¬ æ­£åœ¨éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    sample = dataset[0]
    print(f"   - Image ID: {sample['image_id']}")
    print(f"   - Image size: {sample['image'].size}")
    print(f"   - Image mode: {sample['image'].mode}")
    
    return dataset


def upload_to_huggingface_parquet(dataset: Dataset):
    """
    ä¸Šä¼  Parquet æ ¼å¼æ•°æ®é›†åˆ° HF
    å†…éƒ¨ä¼šè‡ªåŠ¨ï¼š
    1. è½¬æ¢ä¸º .parquet æ–‡ä»¶
    2. åˆ†ç‰‡ï¼ˆå¦‚æœæ•°æ®é‡å¤§ï¼‰
    3. ç”Ÿæˆé…å¥—çš„ dataset_info.json
    """
    print("\nğŸš€ [Hugging Face] æ­£åœ¨ä¸Šä¼  Parquet æ•°æ®é›†...")
    
    try:
        # âš ï¸ å…³é”®å‚æ•°è¯´æ˜ï¼š
        # - private=False: å…¬å¼€æ•°æ®é›†ï¼ˆå¿…é¡»å…¬å¼€æ‰èƒ½æ— é™å­˜å‚¨ï¼‰
        # - max_shard_size="500MB": æ¯ä¸ª Parquet æ–‡ä»¶æœ€å¤§ 500MBï¼ˆè‡ªåŠ¨åˆ†ç‰‡ï¼‰
        dataset.push_to_hub(
            repo_id=HF_REPO,
            private=False,  # å…¬å¼€ä»“åº“
            max_shard_size="500MB",  # è‡ªåŠ¨åˆ†ç‰‡ï¼Œé¿å…å•æ–‡ä»¶è¿‡å¤§
            commit_message=f"Add dataset with {len(dataset)} images"
        )
        
        print("âœ… Hugging Face ä¸Šä¼ å®Œæˆï¼")
        print(f"ğŸ“Š æ•°æ®é›†é“¾æ¥: https://huggingface.co/datasets/{HF_REPO}")
        print(f"\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
        print(f'   from datasets import load_dataset')
        print(f'   ds = load_dataset("{HF_REPO}", split="train", streaming=True)')
        print(f'   # å›½å†…ç”¨æˆ·è¯·å…ˆè®¾ç½®: export HF_ENDPOINT=https://hf-mirror.com')
        
        return True
        
    except Exception as e:
        print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
        
        # å¸¸è§é”™è¯¯æç¤º
        if "authentication" in str(e).lower():
            print("\nğŸ’¡ è¯·å…ˆç™»å½• Hugging Face:")
            print("   huggingface-cli login")
            print("   æˆ–åœ¨ä»£ç å¼€å¤´æ·»åŠ : login(token='hf_...')")
        
        return False


def upload_to_github_simple(code_files: List[Path]):
    """
    ä¸Šä¼ ä»£ç æ–‡ä»¶åˆ° GitHub (ä¿æŒåŸæœ‰é€»è¾‘)
    """
    if not code_files:
        print("\nâœ… [GitHub] æ— éœ€åŒæ­¥ä»£ç æ–‡ä»¶")
        return True
        
    print(f"\nğŸš€ [GitHub] åŒæ­¥ {len(code_files)} ä¸ªä»£ç æ–‡ä»¶...")
    
    work_dir = Path("/tmp/gh_upload_incremental")
    if work_dir.exists(): 
        subprocess.run(["rm", "-rf", str(work_dir)])
    work_dir.mkdir(parents=True)
    
    # å…‹éš†
    print("   ğŸ”„ å…‹éš†ä»“åº“...")
    ssh_url = f"git@github.com:{GITHUB_REPO}.git"
    success, _ = run_cmd(["git", "clone", "-b", GITHUB_BRANCH, ssh_url, str(work_dir)])
    
    if not success:
        https_url = f"https://github.com/{GITHUB_REPO}.git"
        run_cmd(["git", "clone", "-b", GITHUB_BRANCH, https_url, str(work_dir)])
    
    # å¤åˆ¶æ–‡ä»¶
    for f in code_files:
        rel_path = f.relative_to(CURRENT_DIR)
        target = work_dir / rel_path
        target.parent.mkdir(parents=True, exist_ok=True)
        subprocess.run(["cp", str(f), str(target)])
    
    # æäº¤
    run_cmd(["git", "add", "."], cwd=work_dir)
    success, status = run_cmd(["git", "status", "--porcelain"], cwd=work_dir)
    
    if status.strip():
        run_cmd(["git", "commit", "-m", "Update code and docs"], cwd=work_dir)
        success, err = run_cmd(["git", "push"], cwd=work_dir)
        if success:
            print("âœ… GitHub åŒæ­¥å®Œæˆ")
        else:
            print(f"âŒ æ¨é€å¤±è´¥: {err}")
            return False
    else:
        print("âœ… GitHub å·²æ˜¯æœ€æ–°")
    
    return True


def categorize_code_files() -> List[Path]:
    """
    ä»…æ‰«æä»£ç æ–‡ä»¶ï¼ˆæ’é™¤å›¾ç‰‡ï¼‰
    """
    code_files = []
    
    for root, dirs, files in os.walk(CURRENT_DIR):
        # æ’é™¤ç‰¹å®šç›®å½•
        if ".git" in dirs: dirs.remove(".git")
        if "unified-vqa-images" in dirs: dirs.remove("unified-vqa-images")
        if "__pycache__" in dirs: dirs.remove("__pycache__")
        
        for file in files:
            file_path = Path(root) / file
            
            # åªä¸Šä¼ ä»£ç ç›¸å…³æ–‡ä»¶
            if file_path.suffix in {'.py', '.md', '.txt', '.json', '.yaml', '.yml', '.sh'}:
                code_files.append(file_path)
    
    return code_files


if __name__ == "__main__":
    print("="*60)
    print("ğŸš€ Parquet + Streaming æ™ºèƒ½æ‰“åŒ…å·¥å…·")
    print("="*60)
    
    try:
        # â­ æ­¥éª¤ 1: è½¬æ¢ä¸º Parquet æ•°æ®é›†
        dataset = create_parquet_dataset()
        
        # â­ æ­¥éª¤ 2: ä¸Šä¼ åˆ° Hugging Face
        hf_ok = upload_to_huggingface_parquet(dataset)
        
        # æ­¥éª¤ 3: åŒæ­¥ä»£ç åˆ° GitHub
        code_files = categorize_code_files()
        gh_ok = upload_to_github_simple(code_files)
        
        print("\n" + "="*60)
        if hf_ok and gh_ok:
            print("âœ¨ å…¨éƒ¨æå®šï¼æ•°æ®å·²è½¬æ¢ä¸ºé«˜æ•ˆçš„ Streaming æ ¼å¼ï¼")
            print(f"\nğŸ“– å¿«é€Ÿå¼€å§‹:")
            print(f"   # Python ä»£ç ")
            print(f'   from datasets import load_dataset')
            print(f'   ds = load_dataset("{HF_REPO}", split="train", streaming=True)')
            print(f'   for sample in ds.take(5):')
            print(f'       print(sample["image_id"], sample["image"].size)')
        else:
            print("âš ï¸ éƒ¨åˆ†æ“ä½œå¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šæ–¹æ—¥å¿—")
            
    except Exception as e:
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
